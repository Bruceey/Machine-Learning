{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-20T08:58:36.984739800Z",
     "start_time": "2023-08-20T08:58:23.429647200Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Dive_into_deep_learning.d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class PositionWiseFFN(tf.keras.layers.Layer):\n",
    "    \"\"\"The positionwise feed-forward network\"\"\"\n",
    "\n",
    "    def __init__(self, ffn_num_hidens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(ffn_num_hidens)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.dense2 = tf.keras.layers.Dense(ffn_num_outputs)\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T08:58:37.013700100Z",
     "start_time": "2023-08-20T08:58:36.988742100Z"
    }
   },
   "id": "135f596eb1004721"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class AddNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.ln = tf.keras.layers.LayerNormalization(norm_shape)\n",
    "\n",
    "    def call(self, X, Y, **kwargs):\n",
    "        return self.ln(X + self.dropout(Y, **kwargs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T08:58:37.026649200Z",
     "start_time": "2023-08-20T08:58:37.001738Z"
    }
   },
   "id": "ec02b2e3f0639936"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"The Transformer encoder block\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_hiddens, num_heads, dropout, bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def call(self, X, valid_lens, **kwargs):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens, **kwargs), **kwargs)\n",
    "        return self.addnorm2(Y, self.ffn(Y), **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T08:58:37.052615Z",
     "start_time": "2023-08-20T08:58:37.020606700Z"
    }
   },
   "id": "b2f231310671978a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class TransformerEncoder(d2l.Encoder):\n",
    "    \"\"\"The Transformer encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, \n",
    "                 num_hiddens, norm_shape, ffn_num_hiddens, num_heads, \n",
    "                 num_blks, dropout, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = [TransformerEncoderBlock(\n",
    "            key_size, query_size, value_size, num_hiddens, norm_shape, \n",
    "            ffn_num_hiddens, num_heads, dropout, bias) for _ in range(num_blks)]\n",
    "        \n",
    "    def call(self, X, valid_lens, **kwargs):\n",
    "        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(\n",
    "            tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens, **kwargs)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T08:58:37.052615Z",
     "start_time": "2023-08-20T08:58:37.038615700Z"
    }
   },
   "id": "1d3ebaf9d9e91025"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "valid_lens = tf.constant([3, 2])\n",
    "encoder = TransformerEncoder(200, 24, 24, 24, 24, [1, 2], 48, 8, 2, 0.5)\n",
    "d2l.check_shape(encoder(tf.ones((2, 100)), valid_lens, training=False),\n",
    "                (2, 100, 24))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T08:58:45.286550100Z",
     "start_time": "2023-08-20T08:58:37.048615100Z"
    }
   },
   "id": "896fc086d822416d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1521400835.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[7], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    class TransformerDecoderBlock(tf.keras.layers.Layer):\u001B[0m\n\u001B[1;37m                                                         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n",
    "        super().__init__()\n",
    "        self.i = i\n",
    "        self.attention1 = d2l.MultiHeadAttention(num)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-20T09:06:34.464973200Z",
     "start_time": "2023-08-20T09:06:34.358940500Z"
    }
   },
   "id": "d7bec28cd5f21107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5c2922c53048f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
