1.  self.rnns = [d2l.RNNScratch(num_inputs if i==0 else num_hiddens, num_hiddens, sigma)
                        for i in range(num_layers)]
    注意这里的for循环只作用于第一个‘，’前的语句，即 num_inputs if i==0 else num_hiddens

2. tf.stack()用法: Stacks a list of rank-R tensors into one rank-(R+1) tensor.
(1) 在10_3_Deep_RNN.ipynb中的class StackedRNNScratch(d2l.Module): forward方法中，
    outputs = tf.stack(outputs, 0)，由于outputs是num_steps个形状为(batch_size, vocab_size)的列表，
    stack后类型变为tf.Tensor，形状为(num_steps, batch_size, vocab_size)

(2) 在RNNLMScratch的output_layer方法中，
    def output_layer(self, rnn_outputs):
        outputs = [tf.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]
        return tf.stack(outputs, 1)
    由于outputs是num_steps个形状为(batch_size, vocab_size)的列表，
    stack后形状变为(batch_size, num_steps, vocab_size)


3.  RNN最后一层都没有封装，即所有的接口前向传播后返回outputs, state；当return_state=True时才返回state
    outputs的shape为(num_steps, batch_size, vocab_size)，当time_major=True时；
    state是outputs的最后一个元素。以上具体代码针对tensorflow

4. 总结tf.reshape   tf.stack   tf.concate

        outputs = [tf.concat((f, b), -1)
                   for f, b in zip(f_outputs, reversed(b_outputs))]，即在最后一维拼接起来


5. 关于return_state=True的说明
        gru_cells = [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)
                     for _ in range(num_layers)]
        self.rnn = tf.keras.layers.RNN(gru_cells, return_sequences=True,
                                       return_state=True, time_major=True)
        outputs, *state = self.rnn(X, state)
If return_state: return a list of tensors. The first tensor is the output.
The remaining tensors are the last states, each with shape [batch_size, state_size],
where state_size could be a high dimension tensor shape.

另外注意：self.rnn(X, state)   参数分别是inputs, initial_state=None


6. 在10_7的Seq2SeqDecoder类中，将decoder的输入和encoder的输出拼接
           context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))
           # Concat at the feature dimension
           embs_and_context = tf.concat((embs, context), -1)

7. 在10_7中的Seq2Seq类，事后诸葛亮：不知道对不对？？
    def loss(self, Y_hat, Y):
        l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)
        mask = tf.cast(tf.reshape(Y, -1) != self.tgt_pad, tf.float32)
        return tf.reduce_sum(l * mask) / tf.reduce_sum(mask)


10.5节笔记：
    torch.tensor() 只有这一个，不像tf有tf.constant()和tf.Variable()

10.6节笔记
    torch定义forward()方法，tf定义call()方法

10.7节笔记
    # 对模型的每一层定义参数初始化方法
    def init_seq2seq(module):
        """Initialize weights for sequence-to-sequence learning."""
        if type(module) == nn.Linear:
            nn.init.xavier_uniform_(module.weight)
        if type(module) == nn.GRU:
            for param in module._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(module._parameters[param])

   self.apply(init_seq2seq)

   X.t() # pytorch的转置

   # 注意pytorch的默认参数顺序
   self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,
                             dropout=dropout)

   矩阵转置：
      >>> x = torch.randn(2, 3, 5)
      >>> x.size()
      torch.Size([2, 3, 5])
      >>> torch.permute(x, (2, 0, 1)).size()
      torch.Size([5, 2, 3])

      outputs = tf.transpose(self.dense(outputs), (1, 0, 2))

   交叉熵损失：
      pytorch:
          # 对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。
          logsoftmax_func=nn.LogSoftmax(dim=1)
          logsoftmax_output=logsoftmax_func(x_input)
          nllloss_func = nn.NLLLoss()
          nlloss_output = nllloss_func(logsoftmax_output, y_target)

          # 这里的cross_entropy == LogSoftmax + NLLLoss
          F.cross_entropy(Y_hat, Y) # 不管是class indices，还是onehot编码的都是这个

      tensorflow不同:
         # 默认from_logits=False, X是否要经过logits函数(如softmax)处理，False表示不需要，也就是X已经归一化为0-1之间的概率了；
         #    from_logits=True，需要将X先经过logits函数处理，再丢给SparseCategoricalCrossentropy或CategoricalCrossentropy
         fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # 要求Y是class indices
         fn(Y, Y_hat)

         fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # 要求Y是one-hot编码
                  fn(Y, Y_hat)

   交叉熵损失函数的reduction参数：
   pytorch：
      cross_entropy还有一个reduction参数：默认"mean": the sum of the output will be divided by the number of elements in the output
                                   设置为 "None": 计算每个样本的损失，返回一个向量

   tensorflow:
      AUTO: Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE.
      None：计算每个样本的损失，返回一个向量
      默认auto，即根据上下文设置。比如这里：
            def loss(self, Y_hat, Y):
                l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)
                mask = tf.cast(tf.reshape(Y, -1) != self.tgt_pad, tf.float32)
                return tf.reduce_sum(l * mask) / tf.reduce_sum(mask)   # 这里mask是向量，故l也是一个向量（自动设置为None）