1.  self.rnns = [d2l.RNNScratch(num_inputs if i==0 else num_hiddens, num_hiddens, sigma)
                        for i in range(num_layers)]
    注意这里的for循环只作用于第一个‘，’前的语句，即 num_inputs if i==0 else num_hiddens

2. tf.stack()用法: Stacks a list of rank-R tensors into one rank-(R+1) tensor.
(1) 在10_3_Deep_RNN.ipynb中的class StackedRNNScratch(d2l.Module): forward方法中，
    outputs = tf.stack(outputs, 0)，由于outputs是num_steps个形状为(batch_size, vocab_size)的列表，
    stack后类型变为tf.Tensor，形状为(num_steps, batch_size, vocab_size)

(2) 在RNNLMScratch的output_layer方法中，
    def output_layer(self, rnn_outputs):
        outputs = [tf.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]
        return tf.stack(outputs, 1)
    由于outputs是num_steps个形状为(batch_size, vocab_size)的列表，
    stack后形状变为(batch_size, num_steps, vocab_size)


3.  RNN最后一层都没有封装，即所有的接口前向传播后返回outputs, state；当return_state=True时才返回state
    outputs的shape为(num_steps, batch_size, vocab_size)，当time_major=True时；
    state是outputs的最后一个元素。以上具体代码针对tensorflow

4. 总结tf.reshape   tf.stack   tf.concate

        outputs = [tf.concat((f, b), -1)
                   for f, b in zip(f_outputs, reversed(b_outputs))]，即在最后一维拼接起来


5. 关于return_state=True的说明
        gru_cells = [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)
                     for _ in range(num_layers)]
        self.rnn = tf.keras.layers.RNN(gru_cells, return_sequences=True,
                                       return_state=True, time_major=True)
        outputs, *state = self.rnn(X, state)
If return_state: return a list of tensors. The first tensor is the output.
The remaining tensors are the last states, each with shape [batch_size, state_size],
where state_size could be a high dimension tensor shape.

另外注意：self.rnn(X, state)   参数分别是inputs, initial_state=None


6. 在10_7的Seq2SeqDecoder类中，将decoder的输入和encoder的输出拼接
           context = tf.tile(tf.expand_dims(context, 0), (embs.shape[0], 1, 1))
           # Concat at the feature dimension
           embs_and_context = tf.concat((embs, context), -1)

7. 在10_7中的Seq2Seq类，事后诸葛亮：不知道对不对？？
    def loss(self, Y_hat, Y):
        l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)
        mask = tf.cast(tf.reshape(Y, -1) != self.tgt_pad, tf.float32)
        return tf.reduce_sum(l * mask) / tf.reduce_sum(mask)


10.5节笔记：
    torch.tensor() 只有这一个，不像tf有tf.constant()和tf.Variable()

10.6节笔记
    torch定义forward()方法，tf定义call()方法

10.7节笔记
    # 对模型的每一层定义参数初始化方法
    def init_seq2seq(module):
        """Initialize weights for sequence-to-sequence learning."""
        if type(module) == nn.Linear:
            nn.init.xavier_uniform_(module.weight)
        if type(module) == nn.GRU:
            for param in module._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(module._parameters[param])

   self.apply(init_seq2seq)

   X.t() # pytorch的转置